{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_detection_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fByV0lQ5y8gN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SJKjKcv7Wvg",
        "outputId": "1a1df65e-87b0-43d3-e756-b6a5dbe7cb70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df='/content/gdrive/MyDrive/Humans.zip'"
      ],
      "metadata": {
        "id": "faN7A4CV7it6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=zipfile.ZipFile(df,'r')"
      ],
      "metadata": {
        "id": "Qb1G4I4s7mQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.extractall('/content/gdrive/MyDrive/Humans')"
      ],
      "metadata": {
        "id": "A6crgxxB7mTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "m95xTHEO0UaB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSwdQ8EX0Xt5",
        "outputId": "70528f91-1eac-4fd6-b118-b480f6232e9f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "4MKviTCo0ek-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "metadata": {
        "id": "SfW1jsVg0gsl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory('/content/gdrive/MyDrive/Humans',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUxG8H9Y0i8X",
        "outputId": "46da53e6-92e9-4581-9ba5-9e18b9a7187a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7219 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=25,\n",
        "      verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V9eFPDv8UGl",
        "outputId": "6ef84436-6358-4b79-8802-00162d5aba63"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "8/8 [==============================] - 50s 6s/step - loss: 0.0887 - acc: 0.8750\n",
            "Epoch 2/25\n",
            "8/8 [==============================] - 36s 4s/step - loss: 3.8152e-07 - acc: 1.0000\n",
            "Epoch 3/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 2.4420e-08 - acc: 1.0000\n",
            "Epoch 4/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 1.9701e-08 - acc: 1.0000\n",
            "Epoch 5/25\n",
            "8/8 [==============================] - 34s 4s/step - loss: 5.8718e-08 - acc: 1.0000\n",
            "Epoch 6/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 1.8975e-08 - acc: 1.0000\n",
            "Epoch 7/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 1.8590e-08 - acc: 1.0000\n",
            "Epoch 8/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 2.4685e-08 - acc: 1.0000\n",
            "Epoch 9/25\n",
            "4/8 [==============>...............] - ETA: 20s - loss: 8.6001e-09 - acc: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 36s 5s/step - loss: 4.5265e-09 - acc: 1.0000\n",
            "Epoch 10/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 5.8525e-08 - acc: 1.0000\n",
            "Epoch 11/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 5.7272e-10 - acc: 1.0000\n",
            "Epoch 12/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 7.8332e-11 - acc: 1.0000\n",
            "Epoch 13/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 7.8234e-10 - acc: 1.0000\n",
            "Epoch 14/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 1.0338e-10 - acc: 1.0000\n",
            "Epoch 15/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 1.2526e-10 - acc: 1.0000\n",
            "Epoch 16/25\n",
            "8/8 [==============================] - 33s 4s/step - loss: 8.2382e-10 - acc: 1.0000\n",
            "Epoch 17/25\n",
            "8/8 [==============================] - 35s 4s/step - loss: 4.5393e-10 - acc: 1.0000\n",
            "Epoch 18/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 2.8532e-10 - acc: 1.0000\n",
            "Epoch 19/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 4.2919e-10 - acc: 1.0000\n",
            "Epoch 20/25\n",
            "8/8 [==============================] - 34s 4s/step - loss: 3.8340e-10 - acc: 1.0000\n",
            "Epoch 21/25\n",
            "8/8 [==============================] - 34s 4s/step - loss: 3.9415e-10 - acc: 1.0000\n",
            "Epoch 22/25\n",
            "8/8 [==============================] - 32s 4s/step - loss: 2.7965e-11 - acc: 1.0000\n",
            "Epoch 23/25\n",
            "8/8 [==============================] - 35s 4s/step - loss: 9.4088e-11 - acc: 1.0000\n",
            "Epoch 24/25\n",
            "8/8 [==============================] - 34s 4s/step - loss: 1.5695e-10 - acc: 1.0000\n",
            "Epoch 25/25\n",
            "8/8 [==============================] - 37s 5s/step - loss: 4.3421e-10 - acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/gdrive/MyDrive/face_model2.h5')"
      ],
      "metadata": {
        "id": "L3ooWlf3JHN0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16"
      ],
      "metadata": {
        "id": "buW9ghFodMCB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=VGG16(input_shape=(300,300,3),     #Shaoe of our images\n",
        "                 include_top=False,           #Leave out the last fully connected layer\n",
        "                 weights='imagenet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOtN_W2WdME5",
        "outputId": "962c2a3e-86d7-4718-eb89-f42e8364d88b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers"
      ],
      "metadata": {
        "id": "Ce5SwJwsdMHb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "  layer.trainable=False"
      ],
      "metadata": {
        "id": "eRkK2kRqdMKS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten the output latyer to one dimension\n",
        "x = layers.Flatten()(base_model.output)\n",
        "\n",
        "#Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "#Add a dropout rate of 0.5\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "#Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n"
      ],
      "metadata": {
        "id": "RZlTBZkqdMMx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.models.Model(base_model.input, x)"
      ],
      "metadata": {
        "id": "jo4_Fw8_dMPb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), loss='binary_crossentropy',metrics=['acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wModVZNdMR6",
        "outputId": "e665b092-8e6a-417c-af3e-5698d667319d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_generator, steps_per_epoch=8, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke7-nKogdMVS",
        "outputId": "f7a812f2-5231-4dba-e60a-665ceee0d277"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 243s 30s/step - loss: 7.1566e-11 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 237s 29s/step - loss: 3.7662e-10 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 236s 29s/step - loss: 6.9733e-11 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 244s 30s/step - loss: 3.8479e-11 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 239s 30s/step - loss: 3.1308e-09 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 241s 30s/step - loss: 7.8340e-11 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 237s 29s/step - loss: 1.5736e-10 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 236s 29s/step - loss: 8.3459e-11 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 240s 30s/step - loss: 2.7908e-11 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 237s 30s/step - loss: 7.9231e-11 - acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/gdrive/MyDrive/vgg_model.h5')"
      ],
      "metadata": {
        "id": "-cJ--fwWTpbL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wJ2RWLK0tNEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}